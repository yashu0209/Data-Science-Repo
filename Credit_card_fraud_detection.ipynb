# predict fraudulent credit card transactions 

The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.

In this project, you will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

# Reading the data:
df = pd.read_csv('creditcard.csv')
df.shape

#Let us take backup of the original data
cc_data=df

df.info()

df.head()

All the variables except for Time, Amount and the Class variable have been transformed using PCA and we have 28 of these principle components.<br>
This dataset does not contain any null values and is clean for further usage.<br>
The datatypes are also fine since these are principle components and therefore should be decimal values.<br>

# Understanding the distribution of the class variable:
print(df['Class'].value_counts())
print('\n')
print(df['Class'].value_counts(normalize= True))

The data is highly unbalanced with only 0.17% of class 1, i.e the cases where fraud happened.<br>
Let us try to understand the other variables in the dataset.

# Bar plot:
class_df = pd.DataFrame(df['Class'].value_counts())
class_df = class_df.reset_index()
class_df['percentage'] = (class_df['Class']/284807)*100

sns.barplot(x='index', y='percentage', data=class_df)
plt.show()

The dataset is taken from the Kaggle website and it has a total of 2,84,807 transactions, out of which 492 are fraudulent. Since the dataset is highly imbalanced, so it needs to be handled before model building.

# Understanding the Class distribution wrt Amount:
sns.scatterplot(x='Amount', y='Class', data=df, hue='Class')
plt.show()

We can see that all of the fraud transactions have happened for amount less than 5000 euros.<br>
We can also confirm this below:

df[df['Class']==1]['Amount'].describe()

This tells us that the maximum amount for which fraud occurred was 2125.87 euros.

As for the Time variable, it represents the seconds elapsed from the first transaction and therefore does not provide any useful information.<br>
Let us drop this variable from the dataset.

plt.figure(figsize = (20,20))
plt.title('Credit Card Transactions features correlation plot')
correlation = df.corr()
sns.heatmap(correlation,xticklabels=correlation.columns,yticklabels=correlation.columns,linewidths=.1,cmap="Reds")
plt.show()

<font color='red'>As seen from the above heatmap, there is no notable correlation between features V1 to V28. There are some correlations between some of these features and Time (inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V2 and V5).

#Understanding the Data
df.drop(columns='Time', inplace=True)
df.head()

### Train and Test Split:

from sklearn.model_selection import train_test_split

X = df.drop(columns='Class')
y = df['Class']

print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

print(np.sum(y))
print(np.sum(y_train))
print(np.sum(y_test))

#### Understanding the distribution of all the variables in the dataset:
#### V1:

sns.distplot(X_train['V1'])
plt.show()

We can see that the distribution is left skewed here. Let us try to make the distribution normal using the PowerTransformer package.

from sklearn.preprocessing import PowerTransformer

#instantiate 
pt = PowerTransformer(method='yeo-johnson', copy=False) 

#Fit and transform the data to the powertransformer
pt.fit_transform(X_train['V1'].values.reshape(-1,1))

sns.distplot(X_train['V1'])
plt.show()

#### Transforming the Test data for V1:

# Looking at the current distribution:
sns.distplot(X_test['V1'])
plt.show()

# Transforming using the powertransformer "pt" instantiated above:
pt.transform(X_test['V1'].values.reshape(-1,1))

# Looking at the new distribution:
sns.distplot(X_test['V1'])
plt.show()

Now that V1 has been transformed and the skewness has been mitigated to an extent, let us move on with the remaining columns.

var = cc_data.columns.values

i = 0
t0 = cc_data.loc[cc_data['Class'] == 0]
t1 = cc_data.loc[cc_data['Class'] == 1]

sns.set_style('whitegrid')
plt.figure()
fig, ax = plt.subplots(8,4,figsize=(16,28))

for feature in var:
    i += 1
    plt.subplot(8,4,i)
    sns.kdeplot(t0[feature], bw=0.5,label="Class = 0")
    sns.kdeplot(t1[feature], bw=0.5,label="Class = 1")
    plt.xlabel(feature, fontsize=12)
    locs, labels = plt.xticks()
    plt.tick_params(axis='both', which='major', labelsize=12)
plt.show();

From the above plot, we can see that only the Amount column has a skewed distrbution:<br>
Let us change it using powertransform:

# Original distribution of V8:
sns.distplot(X_train['Amount'])
plt.show()

#instantiate 
pt = PowerTransformer(method='yeo-johnson', copy=False) 

#Fit and transform the data to the powertransformer
pt.fit_transform(X_train['Amount'].values.reshape(-1,1))

# Looking at the new distribution:
sns.distplot(X_train['Amount'])
plt.show()

# Transforming the Test Dataset:

# Looking at the current distribution:
sns.distplot(X_test['Amount'])
plt.show()

# Transforming using the powertransformer "pt" instantiated above:
pt.transform(X_test['Amount'].values.reshape(-1,1))

# Looking at the new distribution:
sns.distplot(X_test['Amount'])
plt.show()

### Applying Stratified f-fold Cross validation
Since the dataset is highly imbalanced, stratifed cross validation will be better instead of simple k-fold cross validation.

from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold

kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

# enumerate the splits:
for train_ix, test_ix in kfold.split(X, y):
    # select rows
    X_train_cv, X_test_cv = X.iloc[train_ix], X.iloc[test_ix]
    y_train_cv, y_test_cv = y.iloc[train_ix], y.iloc[test_ix]

print(X_train_cv.shape)
print(X_test_cv.shape)
print(y_train_cv.shape)
print(y_test_cv.shape)

X_train_cv = X_train_cv.values
X_test_cv = X_test_cv.values
y_train_cv = y_train_cv.values
y_test_cv = y_test_cv.values

#### Logistic Regression on the imbalanced dataset:

#Training the model on the train data
from sklearn.linear_model import LogisticRegression
from sklearn import metrics 

logreg = LogisticRegression()
logreg_model = logreg.fit(X_train_cv,y_train_cv)

#Making prediction on the test data
pred_probs_test = logreg_model.predict_proba(X_test_cv)[:,1]

y_pred=logreg.predict(X_test_cv)
y_pred=pd.DataFrame(y_pred)
y_pred= y_pred.rename(columns={ 0 : 'Fraud_Pred'})

# Creating a Confusion matrix:
from sklearn.metrics import confusion_matrix

confusion=confusion_matrix(y_test_cv,y_pred)
confusion

# Calculating the accuracy metrics:
TP = confusion[1,1] # true positive 
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Sensitivity/Recall
TP/ float(TP+FN)

# Precision
TP / float(TP+FP)

We see that in our case with the logistic regression, we get a precison of 0.85 which is good.

#### Random Forest:

# Importing random forest classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Running the random forest
rfc = RandomForestClassifier(class_weight={0:0.1, 1:0.9})

# fit
rfc.fit(X_train_cv,y_train_cv)

# Making predictions
predictions = rfc.predict(X_test_cv)

# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score

# Let's check the report of our default model
print(classification_report(y_test_cv,predictions))

We now have a precision of 0.96 for class 1 which is even better than logistic regression.

#### Boosting:

# importing all modules we use for boosting. 

from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc
import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance
import gc
%matplotlib inline

import os
import warnings
warnings.filterwarnings('ignore')

# fit model on training data:
model = XGBClassifier(class_weight={0:0.1, 1:0.9})
model.fit(X_train_cv, y_train_cv)

# predictions
predictions = model.predict_proba(X_test_cv)
predictions[:10]

# metrics: AUC
metrics.roc_auc_score(y_test_cv, predictions[:,1])

The AUC is 0.97 which is great.

y_pred=model.predict(X_test_cv)
y_pred=pd.DataFrame(y_pred)
y_pred= y_pred.rename(columns={ 0 : 'Fraud_Pred'})

confusion=confusion_matrix(y_test_cv,y_pred)
confusion

TP = confusion[1,1] # true positive 
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Sensitivity/recall:
TP / float(TP+FN)

# Precision
TP / float(TP+FP)

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_cv, y_pred)
roc_auc = auc(false_positive_rate, true_positive_rate)
print (roc_auc)

plt.title('Receiver Operating Characteristic')
plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

We now see that the precison has increased to 0.89<br>
We can see that **XGBoost** gives us the best results and therefore we will proceed with it for hyperparameter tuning.


#### Tuning the hyperparameters using k fold cross vaidation

# hyperparameter tuning with XGBoost

# creating a KFold object 
folds = 3

# specify range of hyperparameters
param_grid = {'learning_rate': [0.2, 0.6], 
             'subsample': [0.3, 0.6, 0.9]}          


# specify model
xgb_model = XGBClassifier(max_depth=2, n_estimators=200)

# set up GridSearchCV()
model_cv = GridSearchCV(estimator = xgb_model, 
                        param_grid = param_grid, 
                        scoring= 'roc_auc', 
                        cv = folds, 
                        verbose = 1,
                        return_train_score=True)      

# fit the model
model_cv.fit(X_train_cv, y_train_cv)       

# cv results
cv_results = pd.DataFrame(model_cv.cv_results_)
cv_results

# convert parameters to int for plotting on x-axis
cv_results['param_learning_rate'] = cv_results['param_learning_rate'].astype('float')
cv_results.head()

# # plotting
plt.figure(figsize=(16,6))

param_grid = {'learning_rate': [0.2, 0.6], 
             'subsample': [0.3, 0.6, 0.9]} 


for n, subsample in enumerate(param_grid['subsample']):
    

    # subplot 1/n
    plt.subplot(1,len(param_grid['subsample']), n+1)
    df = cv_results[cv_results['param_subsample']==subsample]

    plt.plot(df["param_learning_rate"], df["mean_test_score"])
    plt.plot(df["param_learning_rate"], df["mean_train_score"])
    plt.xlabel('learning_rate')
    plt.ylabel('AUC')
    plt.title("subsample={0}".format(subsample))
    plt.legend(['test score', 'train score'], loc='upper left')
    plt.xscale('log')

From the above plots, the subsample size of 0.9 seems to be optimal. Also the learning rate is optimal at 0.2

Building the final model with the chosen hyperparameters.

# chosen hyperparameters
# 'objective':'binary:logistic' outputs probability rather than label, which we need for auc
params = {'learning_rate': 0.2,
          'max_depth': 2, 
          'n_estimators':200,
          'subsample':0.9,
         'objective':'binary:logistic'}

# fit model on training data
model = XGBClassifier(params = params)
model.fit(X_train_cv, y_train_cv)

# predict
y_pred = model.predict_proba(X_test_cv)
y_pred[:10]

# roc_auc
from sklearn import metrics

auc = metrics.roc_auc_score(y_test_cv, y_pred[:, 1])
auc

The final AUC we get by using the optimal hyperparameters for XGBoosting is 0.97

## Class Balancing:

### Random Oversampling

def plot_2d_space(X, y, label='Classes'):   
    colors = ['#1F77B4', '#FF7F0E']
    markers = ['o', 's']
    for l, c, m in zip(np.unique(y), colors, markers):
        plt.scatter(
            X[y==l, 0],
            X[y==l, 1],
            c=c, label=l, marker=m
        )
    plt.title(label)
    plt.legend(loc='upper right')
    plt.show()

X = cc_data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]
y = cc_data['Class']


from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_sample(X, y)

print(X_ros.shape[0] - X.shape[0], 'new random picked points')

plot_2d_space(X_ros.values, y_ros.values, 'Random over-sampling')

X_os_train, X_os_test, y_os_train, y_os_test = train_test_split(X_ros, y_ros, test_size=0.30, random_state=42)

### Logistic Regression model on oversampled dataset

params_lr = {
    'penalty': ['l1', 'l2'],
    'C': [0.08, 0.09, 0.1, 0.11, 0.12],
    'random_state': [78]
}

from sklearn.model_selection import KFold
from sklearn.model_selection import RandomizedSearchCV

kf = KFold(n_splits = 5)
clf_lr = LogisticRegression()
rand_lr = RandomizedSearchCV(clf_lr, params_lr, scoring = 'f1', cv=kf)
rand_lr.fit(X_os_train, y_os_train)

print('LR Best estimator:')
print(rand_lr.best_estimator_)
print('LR Best score:')
print(rand_lr.best_score_ )

predictR = rand_lr.predict(X_os_test)

### Model Evaluation

print(classification_report(y_os_test,predictR))

From the above classification report we are getting precision of 0.97 for class 1 and 0.92 for recall.

print(confusion_matrix(y_os_test,predictR))

### ROC

plt.title('Receiver Operating Characteristic')
plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

### Random Forest

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Running the random forest
rfc = RandomForestClassifier(class_weight={0:0.1, 1:0.9})
rfc.fit(X_os_train, y_os_train)

predictRF = rfc.predict(X_os_test)

### Model Evaluation

print(classification_report(y_os_test,predictRF))

From the above classification report we are getting 1 for both precision and recall.

print(confusion_matrix(y_os_test,predictRF))

### ROC

plt.title('Receiver Operating Characteristic')
plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

### XGBOOST 

# Create the XGB classifier, xgb_model.
xgb_model = XGBClassifier()
# List the default parameters.
print(xgb_model.get_xgb_params())

# Train and evaluate.
xgb_model.fit(X_os_train, y_os_train, eval_metric=['error'], eval_set=[((X_os_train, y_os_train)),(X_os_test, y_os_test)])

from sklearn import metrics
import warnings
warnings.filterwarnings("ignore")

y_pred_xg = xgb_model.predict(X_os_test)  #predicts
print('confusion matrix')
print(metrics.confusion_matrix(y_os_test, y_pred_xg))
print('classification report')
print(metrics.classification_report(y_os_test, y_pred_xg))
print("-----------------------------------------------------------------------------------------")
print("Accuracy is :")
print(metrics.accuracy_score(y_os_test, y_pred_xg))
print('Area under the curve : %f' % (metrics.roc_auc_score(y_os_test, y_pred_xg)))

# Sensitivity/recall:
TP / float(TP+FN)

# Precision
TP / float(TP+FP)

plt.title('Receiver Operating Characteristic')
plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.2])
plt.ylim([-0.1,1.2])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')

### Observations:

<font color='red'>As seen from the above report, recall and precision is 1.
from these observation we can say that the oversampling is better but before that lets also try with SMOTE.


# SMOTE



from collections import Counter
from imblearn.over_sampling import SMOTE 

x = cc_data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]
y = cc_data['Class']

'''Increase the fraud samples from 102 to 500'''

sm = SMOTE(random_state=42,sampling_strategy={1:500})
X_res, y_res = sm.fit_sample(x, y)
print('Resampled dataset shape {}'.format(Counter(y_res)))



plot_2d_space(X_res.values, y_res.values, 'SMOTE')

'''Split the resampled data into train & test data with 70:30 mix'''

xtrain, xtest, ytrain, ytest = train_test_split(X_res, y_res, test_size=0.30, random_state=0)
print('xtrain shape')
print(xtrain.shape)
print('xtest shape')
print(xtest.shape)

'''Random Forest Classifier on resampled data'''

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

rfmodel = RandomForestClassifier()
rfmodel.fit(xtrain,ytrain)
print('model')
print(rfmodel)

ypredrf = rfmodel.predict(xtest)
print('confusion matrix')
print(metrics.confusion_matrix(ytest, ypredrf))
print('classification report')
print(metrics.classification_report(ytest, ypredrf))
print('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))
print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))

From the above classification report we are getting 0.94% of precicion and 0.81% of recall for Random Forest classifier.

'''Gradient Boost Algorithm on resampled data'''

from sklearn import ensemble

params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,
          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(xtrain, ytrain) #trains
y_pred = clf.predict(xtest)  #predicts
print('confusion matrix')
print(metrics.confusion_matrix(ytest, y_pred))
print('classification report')
print(metrics.classification_report(ytest, y_pred))
print("-----------------------------------------------------------------------------------------")
print("Accuracy is :")
print(metrics.accuracy_score(ytest, y_pred))
print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))

Here the recall & precision values have decreased when comapred to Random Forest classifier.

'''Extreme Gradient Boost Algorithm on reduced dimensions data'''

from xgboost.sklearn import XGBClassifier

# Create the XGB classifier, xgb_model.
xgb_model = XGBClassifier()

# List the default parameters.
print(xgb_model.get_xgb_params())

# Train and evaluate.
xgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])

from sklearn import metrics
import warnings
warnings.filterwarnings("ignore")

y_pred = xgb_model.predict(xtest)  #predicts
print('confusion matrix')
print(metrics.confusion_matrix(ytest, y_pred))
print('classification report')
print(metrics.classification_report(ytest, y_pred))
print("-----------------------------------------------------------------------------------------")
print("Accuracy is :")
print(metrics.accuracy_score(ytest, y_pred))
print('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))

From the above classification report we are getting 0.91% of precision and 0.82% of recall for XGB classifier. Recall has slightly increased when comapred to random foreset classifier.

## Logistic Regression Model

kf = KFold(n_splits = 5)
clf_lr = LogisticRegression()
rand_lr = RandomizedSearchCV(clf_lr, params_lr, scoring = 'f1', cv=kf)
rand_lr.fit(X_res, y_res)

print('LR Best estimator:')
print(rand_lr.best_estimator_)
print('LR Best score:')
print(rand_lr.best_score_ )

predict_LR = rand_lr.predict(xtest)

## Model Evaluation

print(classification_report(ytest,predict_LR))

print(confusion_matrix(ytest,predict_LR))

From the above classification report we are getting 0.87% of precicion and 0.69% of recall for Logistic Regression classifier, which is very low when comapred to all other classifier.

### Observation:

<font color='red'>Here we can consider XGB as the best model becuase , our recall is 83%. This means out of all of the data, it correctly identifies 83% of all fraudulent transactions.

Precision answers the question: out of all the transactions predicted to be fraudulent, what percentage were actually fraudulent? In our best model, 92% of all fraudulent transactions are captured. This is a really good metric!

# Building model with ADASYN

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from imblearn.over_sampling import ADASYN
from sklearn.preprocessing import StandardScaler

%%time
pd.options.mode.chained_assignment = None  # default='warn'
ros = ADASYN(n_neighbors = 5, sampling_strategy = 'auto', random_state=123, n_jobs = -1)

X = cc_data.drop(['Class'], axis = 1)
y = cc_data['Class']

# X_data = np.array(X_data)
# y_data = np.array(y_data)

# Create train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

# Convert amount to standar scaler format
ss_obj = StandardScaler()
X_train['nAmount'] = ss_obj.fit_transform(X_train['Amount'].values.reshape(-1,1))
X_train = X_train.drop(['Amount'], axis=1)

# Fit standard scaler object to test data
X_test['nAmount'] = ss_obj.transform(X_test['Amount'].values.reshape(-1,1))
X_test = X_test.drop(['Amount'], axis=1)

X_ros, y_ros = ros.fit_sample(X_train, y_train)
print('Original data shape: {}'.format(Counter(y_train)))
print('Reshaped data shape: {}'.format(Counter(y_ros)))

plot_2d_space(X_ros.values, y_ros.values, 'SMOTE')

print('xtrain shape')
print(X_train.shape)
print('xtest shape')
print(X_test.shape)

### Logistic Regression Model

kf = KFold(n_splits = 5)
clf_lr = LogisticRegression()
rand_lr = RandomizedSearchCV(clf_lr, params_lr, scoring = 'f1', cv=kf)
rand_lr.fit(X_ros, y_ros)

print('LR Best estimator:')
print(rand_lr.best_estimator_)
print('LR Best score:')
print(rand_lr.best_score_ )

predict_LR = rand_lr.predict(X_test)

### Model Evaluation

print(classification_report(y_test,predict_LR))

print(confusion_matrix(y_test,predict_LR))

### Random Forest with ADASYN

'''Random Forest Classifier on resampled data'''

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

rfmodel = RandomForestClassifier()
rfmodel.fit(X_train,y_train)
print('model')
print(rfmodel)

ypredrf = rfmodel.predict(X_test)
print('confusion matrix')
print(metrics.confusion_matrix(y_test, ypredrf))
print('classification report')
print(metrics.classification_report(y_test, ypredrf))
print('Accuracy : %f' % (metrics.accuracy_score(y_test, ypredrf)))
print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, ypredrf)))

### Gradient Boost Algorithm

'''Gradient Boost Algorithm on resampled data'''

from sklearn import ensemble

params = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,
          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train, y_train) #trains
y_pred = clf.predict(X_test)  #predicts
print('confusion matrix')
print(metrics.confusion_matrix(y_test, y_pred))
print('classification report')
print(metrics.classification_report(y_test, y_pred))
print("-----------------------------------------------------------------------------------------")
print("Accuracy is :")
print(metrics.accuracy_score(y_test, y_pred))
print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))

From the above classification report we are getting 0.91% of precicion and 0.74% of recall for Gradinet Boosting classifier.

### XGB

'''Extreme Gradient Boost Algorithm on reduced dimensions data'''

from xgboost.sklearn import XGBClassifier

# Create the XGB classifier, xgb_model.
xgb_model = XGBClassifier()

# List the default parameters.
print(xgb_model.get_xgb_params())

# Train and evaluate.
xgb_model.fit(X_train, y_train, eval_metric=['error'], eval_set=[((X_train, y_train)),(X_test, y_test)])

from sklearn import metrics
import warnings
warnings.filterwarnings("ignore")

y_pred = xgb_model.predict(X_test)  #predicts
print('confusion matrix')
print(metrics.confusion_matrix(y_test, y_pred))
print('classification report')
print(metrics.classification_report(y_test, y_pred))
print("-----------------------------------------------------------------------------------------")
print("Accuracy is :")
print(metrics.accuracy_score(y_test, y_pred))
print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))

From the above classification report we are getting 0.94% of precicion and 0.82% of recall for XGB classifier, which is good when comapred to all other classifier.

We now see that the precison has increased to 0.94%<br>
We can see that **XGBoost** gives us the best results and therefore we will proceed with it for hyperparameter tuning.


#### Tuning the hyperparameters using k fold cross vaidation

# hyperparameter tuning with XGBoost

# creating a KFold object 
folds = 3

# specify range of hyperparameters
param_grid = {'learning_rate': [0.2, 0.6], 
             'subsample': [0.3, 0.6, 0.9]}          


# specify model
xgb_model = XGBClassifier(max_depth=2, n_estimators=200)

# set up GridSearchCV()
model_cv = GridSearchCV(estimator = xgb_model, 
                        param_grid = param_grid, 
                        scoring= 'roc_auc', 
                        cv = folds, 
                        verbose = 1,
                        return_train_score=True)  

# fit the model
model_cv.fit(X_train, y_train)  

# cv results
cv_results = pd.DataFrame(model_cv.cv_results_)
cv_results

# convert parameters to int for plotting on x-axis
cv_results['param_learning_rate'] = cv_results['param_learning_rate'].astype('float')
cv_results.head()

# # plotting
plt.figure(figsize=(16,6))

param_grid = {'learning_rate': [0.2, 0.6], 
             'subsample': [0.3, 0.6, 0.9]} 


for n, subsample in enumerate(param_grid['subsample']):
    # subplot 1/n
    plt.subplot(1,len(param_grid['subsample']), n+1)
    df = cv_results[cv_results['param_subsample']==subsample]

    plt.plot(df["param_learning_rate"], df["mean_test_score"])
    plt.plot(df["param_learning_rate"], df["mean_train_score"])
    plt.xlabel('learning_rate')
    plt.ylabel('AUC')
    plt.title("subsample={0}".format(subsample))
    plt.legend(['test score', 'train score'], loc='upper left')
    plt.xscale('log')


# print best parameter after tuning 
print(model_cv.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(model_cv.best_estimator_) 


From the above plots, the subsample size of 0.6 seems to be optimal. Let us Build the final model with the chosen hyperparameters.

# chosen hyperparameters
# 'objective':'binary:logistic' outputs probability rather than label, which we need for auc
params = {'learning_rate': 0.2,
          'max_depth': 2, 
          'n_estimators':200,
          'subsample':0.6,
         'objective':'binary:logistic'}


# fit model on training data
model = XGBClassifier(params = params)
model.fit(X_train, y_train)


# predict
y_predd = model.predict_proba(X_test)
y_predd[:10]

# roc_auc
from sklearn import metrics

auc = metrics.roc_auc_score(y_test, y_predd[:, 1])
auc


The final AUC we get by using the optimal hyperparameters for XGBoosting is 98.12%


grid_predictions = model_cv.predict(X_test) 
  
# print classification report 
print(classification_report(y_test, grid_predictions)) 


### Observation:

<font color='red'>All of the scores for Random Forest , XGB, Gradient boosting models are very promising for our dataset. Each model has a high true positive rate and a low false positive rate, which is exactly what we’re looking for.
   

# Cost-Benefit Analysis

confusion = metrics.confusion_matrix(y_test, y_pred)
confusion

sns.set(font_scale=1.5)
plt.figure(dpi=60)
sns.heatmap(confusion,annot=True,fmt='d',cmap='winter',linecolor='black',linewidths=0.2)
plt.ylabel('True class')
plt.yticks(fontsize=12)
plt.xlabel('Predicted class')
plt.xticks(fontsize=12)
plt.subtitle('Confusion Matrix')
plt.show()

# Calculating the accuracy metrics:
TP = confusion[1,1] # true positive 
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

Based on the above confusion matrix, our model is going to call **118** people (TP+FP) which are classified as **fraud** transactions.<br><br>
Which means that the bank will call all 118 of these people and verify whether the transaction was fraud or not.<br><br>
Out of 118 people, **111** people will verify that they did not make the transaction and hence the transaction was fraudulent, whereas **7** people will verify the transaction to be genuine.<br>

Furthermore, there are **25** transactions which are wrongly predicted as not-fraud. And these transactions will add up as loss to the bank.<br>

Assuming the calling cost to be **10 rupees per call**, the savings and cost to the bank will be as follows:<br>
- Savings(S1) = 111 * 10 = 1110
- Cost1 (C1) = 7 * 10 = 70
- Cost2 (C2) = 25 * 10 = 250
<br>

So the total savings= (TP x cost of each transaction (correct predictions) -[ (TP+FP) x 10 + FN x Cost of all transactions (incorrect predictions)] ):<br>
- Total Savings = S1 - (C1 + C2) = 1110 - (70 + 250) = **790** 

<br>
Our model ends up saving 790 rupees for the bank.


